Apache Kafka is a high-throughput distributed messaging system.

At first, system architectures start simple: we have some source system a target system and data is passed from source to target:

Figure Here

As requirements increase, the system gets increasingly complex and more source and target systems arise. In the worst case scenario, we have `n` source systems and `n` target systems where each source system must communicate with every target system. Fundamentally, this requires `n^2` integrations:

Figure Here

Moreover, each integration comes with difficulties:
- Protocol: how the data is transported (TCP, HTTP, REST, FTP, JDBC...)
- Data format: how the data is parsed (Binary, CSV, JSON, Avro...)
- Data schema and evolution: how the data is shaped and may change

Ultimately, this is error prone, time consuming, and not scalable.

Apache Kafka helps decouple data streams and systems. Going back to the previous example, using a Kafka centered architecture, if we still had `n` source systems and `n` target systems where each source system has to communicate with every target system we would only need `n+n` integrations:

Figure Here

Figure Here

About Kafka:
- Created by LinkedIn, now open source and mainly maintained by Confluent
- Distributed, resilient architecture, fault tolerant
- Horizontal scalability:
  - Can scale to 100s of brokers
  - Can scale to millions of messages per second
- High performance (<10ms -- i.e., real time)

Use cases:
- Messaging system
- Activity tracking
- Metric gathering from various locations
- Application log gathering
- Stream processing (e.g., with the Kafka streams API or Spark)
- De-coupling of system dependencies
- Integration with Spark, Flink, Storm, Hadoop, and other Big Data technologies

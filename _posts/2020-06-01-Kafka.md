Apache Kafka is a high-throughput distributed messaging system.

# Overview 

At first, system architectures start simple: we have some source system a target system and data is passed from source to target:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+5.54.47+PM.png)

As requirements increase, the system gets increasingly complex and more source and target systems arise. In the worst case scenario, we have `n` source systems and `n` target systems where each source system must communicate with every target system. Fundamentally, this requires `n^2` integrations:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.00.54+PM.png)

Moreover, each integration comes with difficulties:
- Protocol: how the data is transported (TCP, HTTP, REST, FTP, JDBC...)
- Data format: how the data is parsed (Binary, CSV, JSON, Avro...)
- Data schema and evolution: how the data is shaped and may change

Ultimately, this is error prone, time consuming, and not scalable.

Apache Kafka helps decouple data streams and systems. Going back to the previous example, using a Kafka centered architecture, if we still had `n` source systems and `n` target systems where each source system has to communicate with every target system we would only need `n+n` integrations:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.04.43+PM.png)

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.11.47+PM.png)

About Kafka:
- Created by LinkedIn, now open source and mainly maintained by Confluent
- Distributed, resilient architecture, fault tolerant
- Horizontal scalability:
  - Can scale to 100s of brokers
  - Can scale to millions of messages per second
- High performance (<10ms -- i.e., real time)

Use cases:
- Messaging system
- Activity tracking
- Metric gathering from various locations
- Application log gathering
- Stream processing (e.g., with the Kafka streams API or Spark)
- De-coupling of system dependencies
- Integration with Spark, Flink, Storm, Hadoop, and other Big Data technologies

# Kafka Theory 

## Topics, partitions and offsets

- __Topic:__ a particular stream of data
  - You can have as many topics as you want
  - A topic is identified by its name
- Split into __partitions__
  - Each partition is ordered (e.g., partition 0, partition 1, partition 2, etc..)
  - Each message within a partition gets an incremental id, called __offset__

To identify a specific message (or "record") in Kafka, you will need to know its __topic__, __partition__, and __offset__.

Note that __offset__ by itself has to meaning. For example, offset 3 in partition 0 does not represent the same thing as offset 3 in partition 1.

Example scenario:

Say you have a fleet of trucks and each truck reports its GPS position to Kafka. In this scenario, we can have a topic `truck_gps` that contains the position of all trucks. Each truck will send a message to this Kafka topic every 20 seconds. The message is composed of: truck ID, latitude, longitude. We can choose to create the topic with 10 partitions (completely arbitrary in this case). Having this topic, services can subscribe to it and use its data. For example, a location dashboard can use this data to show the truck's position. A notification service might use this data to notify when a truck has been driving too long or if it might be low on gas.

Food for thought: why use Kafka in this scenario? Why not use RabbitMQ? What about AWS SQS?

Order in a Kafka topic is only guaranteed within a partition (not across partitions).

Data is kept only for a limited time (default is one week, however __retention period__ can be modified).

Once data is written to a partition, it cannot be changed (immutability).

Data is assigned randomly to a partition unless a key is provided.

# Kafka CLI

Apache ZooKeeper is a required Kafka dependency.

To start ZooKeeper:

In the Kafka installation directory (e.g., `/usr/local/kafka_2.12-2.5.0`), run:

`zookeeper-server-start.sh config/zookeeper.properties`.

To start Kafka:

Assuming the Kafka binaries are in your `$PATH`, run:

`kafka-server-start.sh server.properties`. 

If the Kafka binaries are not in `$PATH`, you need to navigate to the Kafka installation directory (e.g., `/usr/local/kafka_2.12-2.5.0`).

To create a Kafka topic:

`kafka-topics.sh --zookeeper localhost:2181 --topic first_topic --create --partitions 3 --replication-factor 1`

Explanation:
- `--zookeeper localhost:2181` points to the associated ZooKeeper instance (remember, ZooKepeeper is a required dependency)
- `--topic --create first_topic` tells Kafka to create a topic named `first_topic`
- `--partitions 3` tells Kafka to split `first_topic` into `3` partitions
- `--replication-factor 1` requests a replication factor of `1`

**Note:** replication factor cannot be greater than the number of brokers. If replication factor > number of brokers, one or more insync replica partition will be on the same broker as the lead partition. This is counter productive. The purpose of a replica partition is to be live if the lead partition dies.. if they are on the same broker (i.e., machine / server), then we loose both anyways.

To describe a Kafka topic:

`kafka-topics.sh --zookeeper localhost:2181 --topic second_topic --describe`

To list all Kafka topics:

`kafka-topics.sh --zookeeper localhost:2181 --list`

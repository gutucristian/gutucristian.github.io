Apache Kafka is a high-throughput distributed messaging system.

# Overview 

At first, system architectures start simple: we have some source system a target system and data is passed from source to target:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+5.54.47+PM.png)

As requirements increase, the system gets increasingly complex and more source and target systems arise. In the worst case scenario, we have `n` source systems and `n` target systems where each source system must communicate with every target system. Fundamentally, this requires `n^2` integrations:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.00.54+PM.png)

Moreover, each integration comes with difficulties:
- Protocol: how the data is transported (TCP, HTTP, REST, FTP, JDBC...)
- Data format: how the data is parsed (Binary, CSV, JSON, Avro...)
- Data schema and evolution: how the data is shaped and may change

Ultimately, this is error prone, time consuming, and not scalable.

Apache Kafka helps decouple data streams and systems. Going back to the previous example, using a Kafka centered architecture, if we still had `n` source systems and `n` target systems where each source system has to communicate with every target system we would only need `n+n` integrations:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.04.43+PM.png)

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.11.47+PM.png)

About Kafka:
- Created by LinkedIn, now open source and mainly maintained by Confluent
- Distributed, resilient architecture, fault tolerant
- Horizontal scalability:
  - Can scale to 100s of brokers
  - Can scale to millions of messages per second
- High performance (<10ms -- i.e., real time)

Use cases:
- Messaging system
- Activity tracking
- Metric gathering from various locations
- Application log gathering
- Stream processing (e.g., with the Kafka streams API or Spark)
- De-coupling of system dependencies
- Integration with Spark, Flink, Storm, Hadoop, and other Big Data technologies

# Kafka Theory 

# Kafka CLI

Apache ZooKeeper is a required Kafka dependency.

To start ZooKeeper:

In the Kafka installation directory (e.g., `/usr/local/kafka_2.12-2.5.0`), run:

`zookeeper-server-start.sh config/zookeeper.properties`.

To start Kafka:

Assuming the Kafka binaries are in your `$PATH`, run:

`kafka-server-start.sh server.properties`. 

If the Kafka binaries are not in `$PATH`, you need to navigate to the Kafka installation directory (e.g., `/usr/local/kafka_2.12-2.5.0`).

To create a Kafka topic:

`kafka-topics.sh --zookeeper localhost:2181 --topic --create first_topic --partitions 3 --replication-factor 1`

Explanation:
- `--zookeeper localhost:2181` points to the associated ZooKeeper instance (remember, ZooKepeeper is a required dependency)
- `--topic --create first_topic` tells Kafka to create a topic named `first_topic`
- `--partitions 3` tells Kafka to split `first_topic` into `3` partitions
- `--replication-factor 1` requests a replication factor of `1`

**Note:** replication factor cannot be greater than the number of brokers. If replication factor > number of brokers, one or more insync replica partition will be on the same broker as the lead partition. This is counter productive. The purpose of a replica partition is to be live if the lead partition dies.. if they are on the same broker (i.e., machine / server), then we loose both anyways.

To describe a Kafka topic:

`kafka-topics.sh --zookeeper localhost:2181 --topic second_topic --describe`

To list all Kafka topics:

`kafka-topics.sh --zookeeper localhost:2181 --list`

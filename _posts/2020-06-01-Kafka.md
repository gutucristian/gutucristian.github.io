Apache Kafka is a high-throughput distributed messaging system.

# Overview 

At first, system architectures start simple: we have some source system a target system and data is passed from source to target:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+5.54.47+PM.png)

As requirements increase, the system gets increasingly complex and more source and target systems arise. In the worst case scenario, we have `n` source systems and `n` target systems where each source system must communicate with every target system. Fundamentally, this requires `n^2` integrations:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.00.54+PM.png)

Moreover, each integration comes with difficulties:
- Protocol: how the data is transported (TCP, HTTP, REST, FTP, JDBC...)
- Data format: how the data is parsed (Binary, CSV, JSON, Avro...)
- Data schema and evolution: how the data is shaped and may change

Ultimately, this is error prone, time consuming, and not scalable.

Apache Kafka helps decouple data streams and systems. Going back to the previous example, using a Kafka centered architecture, if we still had `n` source systems and `n` target systems where each source system has to communicate with every target system we would only need `n+n` integrations:

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.04.43+PM.png)

![](https://s3.amazonaws.com/gutucristian.github.io/Kafka/Screen+Shot+2020-06-01+at+6.11.47+PM.png)

About Kafka:
- Created by LinkedIn, now open source and mainly maintained by Confluent
- Distributed, resilient architecture, fault tolerant
- Horizontal scalability:
  - Can scale to 100s of brokers
  - Can scale to millions of messages per second
- High performance (<10ms -- i.e., real time)

Use cases:
- Messaging system
- Activity tracking
- Metric gathering from various locations
- Application log gathering
- Stream processing (e.g., with the Kafka streams API or Spark)
- De-coupling of system dependencies
- Integration with Spark, Flink, Storm, Hadoop, and other Big Data technologies

# Kafka Theory 

## Topics, partitions and offsets

In Kafka, a __topic__ is a particular stream of data. Assuming storage cost isn't an issue, you can have as many topics as you want. Each topic is identified by a name. Topics can be split into one or more __partitions__ (similar to database sharding). Partitions are ordered (e.g., partition 0, partition 1, partition 2, etc..). Each message within a partition gets an incremental id, called __offset__.

Note that __offset__ and __partition__ alone have no meaning. To identify a specific message (or "record") in Kafka, you will need to know its __topic__, __partition__, and __offset__.

Example scenario:

Say you have a fleet of trucks and each truck reports its GPS position to Kafka. In this scenario, we can have a topic `truck_gps` that contains the position of all trucks. Each truck will send a message to this Kafka topic every 20 seconds. The message is composed of: truck ID, latitude, longitude. We can choose to create the topic with 10 partitions (completely arbitrary in this case). Having this topic, services can subscribe to it and use its data. For example, a location dashboard can use this data to show the truck's position. A notification service might use this data to notify when a truck has been driving too long or if it might be low on gas.

Food for thought: why use Kafka in this scenario? Why not use RabbitMQ? What about AWS SQS?

Order in a Kafka topic is only guaranteed within a partition (not across partitions).

Data is kept only for a limited time (default is one week, however __retention period__ can be modified). When retention period is reached and data is cleared, __offsets__ remain (i.e., they DO NOT go back to 0).

After data is written to a partition, it cannot be changed (__immutability__).

Data is assigned randomly to a partition unless a key is provided.

## Brokers and Topics

A Kafka __cluster__ is composed of multiple __brokers__ (servers). Each broker is identified by an ID (integer). Each broker contains __some__ topic partitions but __not all__ (Kafka is a distributed system afterall). After connecting to one broker from a cluster (called a __bootstrap broker__), you will be connected to entire cluster. A good number to get started is 3 brokers (will explain why 3 later). However, there are cases where clusters can reach over 100 brokers.

When you create a topic, Kafka will automatically distributed it across all your brokers. For example, if we have three brokers (e.g., broker 101, 102, 103) and we create a topic "A" with three partitions, then each broker will hold one partition. So broker 101 will hold topic A partition 0, broker 102 will hold topic A partition 1, broker 103 will hold topic A parition 2. 

Continuing the previous example, lets say we also create another topic "B" with 2 partitions. If we also create a topic "B" but with two partitions, then broker 101 could hold topic B partition 1 and broker 102 would hold topic B partition 0. The idea is that __partitions are distributed across brokers__.

## Topic Replication

The idea behind having multiple brokers and topic replication is to guarantee __fault tolerance__. Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of some of its components. 

With Kafka, we want to guarantee that the system continues to run as expected even if some brokers go down. It is recommended that topic replication should be > 1 (usually between 2 and 3) -- 3 being the "gold standard".

Topic replication is when a topic (and all it's partitions) are _replicated_ across brokers. This way if any one broker goes down, another broker can serve the data.

Replication factor determines the number of replications each partition has, this allows Kafka to __automatically failover__ to these replicas when a server in the cluster fails so that messages remain available in case of failures

A broker can only host a single replica for a given topic partition -- it gains us nothing to hold two copies of the same partition on the same broker.

As mentioned before, partition replicas are distributed across brokers and since one broker should keep one replica that means __we can't have more replicas than the number of brokers__.

So if our cluster has 3 brokers, the maximum replication factor we can have is 3.

Therefore: __max replication factor <= brokers number__

This is also meant to determine __min.insync.replicas__ which always must be less than or equal to replication-factor: __min.insync.replicas <= replication factor__

__min.insync.replicas__ decide how many brokers that must acknowledge a producer when a message is sent with __acks=all__. min.insync.replicas increase the __durability__ of your data since you will know that once the producer gets an acknowledge, you can be certain that the data is stored in configured numbers of brokers. However, this setting might completely stop your service during a rolling restart. If you have a cluster with three nodes and you set min.insync.replicas to three, your producers will require acks from three brokers, but this is not possible when one broker is down, which it is during the restart. You will not be able to produce any messages until the restart has finished. In these cases is a regular restart (where you are taking down all brokers at the same) faster than a rolling restart ([source](https://www.cloudkarafka.com/blog/2018-10-11-rolling-restart-of-apache-kafka.html#:~:text=A%20rolling%20restart%20means%20that,and%20with%20no%20message%20lost.)).

While deciding replication factor consider below points as well:

A): Broker Size Replication factor directly impacts the overall broker disk size

So a high replication factor requires more disk size

B)Large Number of Partition replication: In case of a large number of partitions replication extra latency is added.

## Concept of Partition Leader

- At any one time only ONE broker can be a leader for a given partition
- __Only that leader can receive and serve data for a partition__
- The other brokers will synchronize the data
- Therefore, each partition has one leader and multiple ISR (in-sync replicas)

ZooKeeper is the one who decides which broker is a partition leader for which topic. If a broker goes down, then ZooKeeper will perform "elections" to decide the next bootstrap broker for the cluster as well as new partition leaders for each topic partition.

## Producers and Message Keys

- Producers write data to topics (which may be broken up into multiple partitions)
- Producers automatically know which broker and partition to write to for a given topic
- In case of a broker failure, producers will automatically recover
- If a producer published a record _without a key_ (`key`=`null`) then Kafka will write messages round robin to available partitions for the given topic (simple, automatic load balancing done by Kafka)
- If the producer specifies a key when writing to a topic, then Kafka will always write records with this key to the same partition for this topic (Kafka uses a hashing mechanism based on given key and number of topic partitions to determine which topic to write to -- similar to hashing concept in hashtable data structure)
- `key` can be a string or a number
- Producers can choose to receive acknowledgement of data writes
  - acks=0 : producer won't wait for acknowledgement (possible data loss)
  - acks=1 : producer will wait for __partition leader acknowledgement__ (limited data loss)
  - acks=all : __partition leader + replicas acknowledgement__ (no data loss)

Very important: you only need to connect to one broker (any broker) and just provide the topic name you want to write to. Kafka Clients will route your data to the appropriate brokers and partitions for you!

## Consumers

- Consumers read data from a topic
- Consumers automatically know which broker to read from (this is done for us "auto-magically" by Kafka)
- In case of broker failures, consumers know how to recover
- Data is read in order __within each partition__ but read order is __NOT__ guaranteed across partitions

Very important: you only need to connect to one broker (any broker) and just provide the topic name you want to read from. Kafka will route your calls to the appropriate brokers and partitions for you!

## Consumer Groups

- Consumers read data in consumer groups
- A consumer can read data from multiple partitions but a partition can only have one consumer
- If you have more consumers than partitions, some consumers will be inactive
- Consumer groups is what enables parallel processing in Kafka
- Because of consumer groups Kafka can be used either as a pub-sub or RabbitMQ queue like system
- If we want our Kafka system to process messages once (similar to a queue architecture) then we can have only one consumer group per topic. If we want a pub-sub broadcast architecture, then we can have multiple consumer groups per topic and each consumer group will separately consume messages from that topic in parallel (see more on Hussein Nassar's [video](https://www.youtube.com/watch?v=R873BlNVUB4&list=PLQnljOFTspQUBSgBXilKhRMJ1ACqr7pTr) on Kafka)

## Consumer Offsets

- Kafka stores the offsets at which a consumer group has been reading
- The offsets are commited live in a Kafka topic named consumer_offsets
- When a consumer in a group has processed data received from Kafka, it should be committing the offsets
- If a consumer dies, it will be able to read back from where it left off thanks to the committed consumer offsets

## Delivery Semantics for Consumers

- Consumers choose when to commit offsets
- At most once
  - Offsets are committed as soon as the message is received
  - If the processing goes wrong, the message will be lost
- At least once
  - Offsets are committed after the message is processed
  - If the processing goes wrong, the message will be read again
  - This can result in duplicate processing of messages, so make sure you processing is __idempotent__ (i.e., processing the message again won't impact the result / outcome & your system)
- Exactly once
  - Can be achieved using Kafka Streams API

## Kafka Broker Discovery

- Every Kafka broker is also called a __bootstrap broker__
- This means you only need to connect to one broker and you will be connected to the entire cluster
- Each broker knows about all brokers, topics, and partitions (metadata)

So workflow is as follows:
1. Consumer submits connection + metadata request to some broker in the cluster
2. Consumer received a list of all brokers
3. Consumer can connect to the needed broker which is the partition leader for the topic and partition that the consumer wants to read from

## Zookeeper

- Zookeeper manages brokers (keeps a list of them)
- Zookeeper helps in performing leader elections for partitions
- Zookeeper sends notifications to Kafka in case of changes (e.g., new topic, broker dies, broker comes up, delete topics, etc...)
- Zookeeper by design operates with an odd number of servers (3, 5, 7, ...)
- Zookeeper has a leader (handle writes) the rest of the servers are followers (handle reads)

## Kafka Guarantees

- Messages are appended to a topic-partition in the order they are sent
- Consumers read messages in the order stored in a topic-partition
- With a replication factor of `N`, producers and consumers can tolerate up to `N-1` brokers being down (given that `min.insync.replicas` != `N` -- in this case we would need to always have `N` brokers up to `ack` a given write)
- Having a replication factor of `3` and a `min.insync.replica` of `2` would allow us to:
  - Allow for one brokers to be taken down for maintainance
  - Allow for one broker to be taken down unexpectedly
- As long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to same partition (due to hashing -- `record partition num. == key % num. of partitions`)

# Kafka CLI

Apache ZooKeeper is a required Kafka dependency.

To start ZooKeeper:

In the Kafka installation directory (e.g., `/usr/local/kafka_2.12-2.5.0`), run:

`zookeeper-server-start.sh config/zookeeper.properties`.

To start Kafka:

Assuming the Kafka binaries are in your `$PATH`, run:

`kafka-server-start.sh server.properties`. 

If the Kafka binaries are not in `$PATH`, you need to navigate to the Kafka installation directory (e.g., `/usr/local/kafka_2.12-2.5.0`).

To create a Kafka topic:

`kafka-topics.sh --zookeeper localhost:2181 --topic first_topic --create --partitions 3 --replication-factor 1`

Explanation:
- `--zookeeper localhost:2181` points to the associated ZooKeeper instance (remember, ZooKepeeper is a required dependency)
- `--topic --create first_topic` tells Kafka to create a topic named `first_topic`
- `--partitions 3` tells Kafka to split `first_topic` into `3` partitions
- `--replication-factor 1` requests a replication factor of `1`

**Note:** replication factor cannot be greater than the number of brokers. If replication factor > number of brokers, one or more insync replica partition will be on the same broker as the lead partition. This is counter productive. The purpose of a replica partition is to be live if the lead partition dies.. if they are on the same broker (i.e., machine / server), then we loose both anyways.

To describe a Kafka topic:

`kafka-topics.sh --zookeeper localhost:2181 --topic second_topic --describe`

To list all Kafka topics:

`kafka-topics.sh --zookeeper localhost:2181 --list`
